#!/usr/bin/env python3
"""
ä¸ºè¯¾ç¨‹4åˆ›å»ºçŸ¥è¯†åº“çš„ä¸“ç”¨è„šæœ¬
"""

import os
import sys
import shutil
import json
import time
import hashlib
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_knowledge_base_for_course_4():
    """ä¸ºè¯¾ç¨‹4åˆ›å»ºçŸ¥è¯†åº“"""
    
    course_id = "4"
    
    # å®šä¹‰è·¯å¾„
    materials_dir = os.path.join(project_root, "backend", "uploads", "materials", course_id)
    knowledge_base_dir = os.path.join(project_root, "backend", "uploads", "knowledge_base", course_id)
    
    print(f"=== ä¸ºè¯¾ç¨‹ {course_id} åˆ›å»ºçŸ¥è¯†åº“ ===")
    print(f"ææ–™ç›®å½•: {materials_dir}")
    print(f"çŸ¥è¯†åº“ç›®å½•: {knowledge_base_dir}")
    
    # æ£€æŸ¥ææ–™ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(materials_dir):
        print(f"âŒ ææ–™ç›®å½•ä¸å­˜åœ¨: {materials_dir}")
        return False
    
    # åˆ›å»ºçŸ¥è¯†åº“ç›®å½•
    os.makedirs(knowledge_base_dir, exist_ok=True)
    
    # å¤åˆ¶æ–‡ä»¶åˆ°çŸ¥è¯†åº“ç›®å½•
    copied_files = []
    for filename in os.listdir(materials_dir):
        source_file = os.path.join(materials_dir, filename)
        target_file = os.path.join(knowledge_base_dir, filename)
        
        if os.path.isfile(source_file):
            try:
                shutil.copy2(source_file, target_file)
                copied_files.append(filename)
                print(f"âœ“ å¤åˆ¶æ–‡ä»¶: {filename}")
            except Exception as e:
                print(f"âŒ å¤åˆ¶æ–‡ä»¶å¤±è´¥ {filename}: {e}")
    
    print(f"âœ“ å¤åˆ¶äº† {len(copied_files)} ä¸ªæ–‡ä»¶åˆ°çŸ¥è¯†åº“ç›®å½•")
    
    # åˆ›å»ºçŠ¶æ€æ–‡ä»¶
    status_file = os.path.join(knowledge_base_dir, "processing_status.json")
    status_data = {
        "course_id": course_id,
        "source_dir": materials_dir,
        "target_dir": knowledge_base_dir,
        "copied_files": copied_files,
        "processed_at": int(time.time()),
        "status": "ready_for_processing"
    }
    
    with open(status_file, 'w', encoding='utf-8') as f:
        json.dump(status_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ“ çŠ¶æ€æ–‡ä»¶å·²åˆ›å»º: {status_file}")
    
    # ç°åœ¨è¿è¡Œå‘é‡æ•°æ®åº“åˆ›å»º
    print("\n=== å¼€å§‹åˆ›å»ºå‘é‡æ•°æ®åº“ ===")
    
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_chroma import Chroma
        from chromadb.config import Settings
        
        # å®šä¹‰EmbeddingFunctionç±»
        class EmbeddingFunction:
            def __init__(self):
                self.logger = logging.getLogger(__name__)
            
            def embed_documents(self, texts):
                # ç®€åŒ–çš„embeddingå‡½æ•°ï¼Œè¿”å›1024ç»´å‘é‡
                embeddings = []
                for text in texts:
                    # è¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„embedding API
                    # æš‚æ—¶è¿”å›éšæœºå‘é‡ä½œä¸ºç¤ºä¾‹
                    import random
                    embedding = [random.uniform(-1, 1) for _ in range(1024)]
                    embeddings.append(embedding)
                return embeddings
            
            def embed_query(self, text):
                import random
                return [random.uniform(-1, 1) for _ in range(1024)]
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        all_docs = []
        for filename in copied_files:
            file_path = os.path.join(knowledge_base_dir, filename)
            print(f"å¤„ç†æ–‡ä»¶: {filename}")
            
            try:
                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
                _, ext = os.path.splitext(filename)
                ext = ext.lower()
                
                if ext == '.pdf':
                    loader = PyMuPDFLoader(file_path)
                elif ext in ['.docx', '.doc']:
                    loader = Docx2txtLoader(file_path)
                elif ext in ['.md', '.markdown']:
                    # ç›´æ¥è¯»å–Markdownæ–‡ä»¶
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    from langchain_core.documents import Document
                    docs = [Document(page_content=content, metadata={"source": filename})]
                else:
                    # è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹
                    print(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filename}")
                    continue
                
                if ext not in ['.md', '.markdown']:
                    docs = loader.load()
                
                for doc in docs:
                    doc.metadata['source'] = filename
                
                all_docs.extend(docs)
                print(f"âœ“ åŠ è½½å®Œæˆ: {filename} ({len(docs)} ä¸ªæ–‡æ¡£)")
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {filename}: {e}")
                continue
        
        if not all_docs:
            print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£")
            return False
        
        print(f"âœ“ æ€»å…±åŠ è½½äº† {len(all_docs)} ä¸ªæ–‡æ¡£")
        
        # åˆ†å‰²æ–‡æ¡£
        print("åˆ†å‰²æ–‡æ¡£...")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
        splits = text_splitter.split_documents(all_docs)
        
        # æ·»åŠ å”¯ä¸€ID
        for i, doc in enumerate(splits):
            doc.metadata['chunk_id'] = f"chunk_{i}"
        
        print(f"âœ“ åˆ†å‰²å®Œæˆï¼Œå…± {len(splits)} ä¸ªæ–‡æœ¬å—")
        
        # åˆ›å»ºå‘é‡æ•°æ®åº“
        print("åˆ›å»ºå‘é‡æ•°æ®åº“...")
        persist_dir = os.path.join(knowledge_base_dir, "vectordb")
        os.makedirs(persist_dir, exist_ok=True)
        
        embedding_function = EmbeddingFunction()
        
        # åˆ›å»ºChromaå‘é‡æ•°æ®åº“
        vectorstore = Chroma(
            persist_directory=persist_dir,
            embedding_function=embedding_function,
            client_settings=Settings(anonymized_telemetry=False)
        )
        
        # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
        vectorstore.add_documents(splits)
        print(f"âœ“ å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆ: {persist_dir}")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_path = os.path.join(knowledge_base_dir, 'processed_files.json')
        processed_files = {}
        
        for filename in copied_files:
            file_path = os.path.join(knowledge_base_dir, filename)
            if os.path.exists(file_path):
                file_hash = hashlib.md5(open(file_path, 'rb').read()).hexdigest()
                processed_files[filename] = {
                    'hash': file_hash,
                    'processed_at': int(time.time()),
                    'chunks': len([s for s in splits if s.metadata.get('source') == filename])
                }
        
        with open(metadata_path, 'w') as f:
            json.dump(processed_files, f, indent=2)
        
        print(f"âœ“ å…ƒæ•°æ®ä¿å­˜å®Œæˆ: {metadata_path}")
        
        # æ›´æ–°çŠ¶æ€æ–‡ä»¶
        status_data['status'] = 'completed'
        status_data['vector_db_path'] = persist_dir
        status_data['total_chunks'] = len(splits)
        status_data['completed_at'] = int(time.time())
        
        with open(status_file, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, indent=2, ensure_ascii=False)
        
        print("=== çŸ¥è¯†åº“åˆ›å»ºå®Œæˆ ===")
        print(f"âœ“ è¯¾ç¨‹ {course_id} çš„çŸ¥è¯†åº“å·²æˆåŠŸåˆ›å»º")
        print(f"âœ“ å‘é‡æ•°æ®åº“ä½ç½®: {persist_dir}")
        print(f"âœ“ æ€»æ–‡æœ¬å—æ•°: {len(splits)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºå‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = create_knowledge_base_for_course_4()
    if success:
        print("\nğŸ‰ è¯¾ç¨‹4çš„çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸï¼ç°åœ¨å¯ä»¥æµ‹è¯•RAGåŠŸèƒ½äº†ã€‚")
    else:
        print("\nâŒ çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚") 