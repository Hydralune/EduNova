新增对已有文件的校验,

太好了！
那我来给你写一个「**自动增量更新版本**」的 `create_or_update_db`，思路是：

✅ 自动检测已有向量库
✅ 对比已有文档的 source（存在就跳过）
✅ 只对新增的文档做切块 + 调用 embedding API
✅ 用 `add_documents` 把新的块追加进去
✅ 避免重复嵌入，节省 token 和时间

---

## 🚀 **自动增量版**

请看下面完整示例（替换你现在的 `create_db` 和 `load_knowledge_db` 就行）：

```python
import os
import re
from typing import List, Dict, Optional
from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredMarkdownLoader, UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
import sys

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from embedding_util import get_embedding


class DeepSeekEmbeddingFunction:
    """Custom embedding function using DeepSeek API."""

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        result = get_embedding(texts)
        embeddings = [item["embedding"] for item in result["data"]]
        return embeddings

    def embed_query(self, text: str) -> List[float]:
        result = get_embedding(text)
        return result["data"][0]["embedding"]


def file_loader(file_path: str) -> List[Dict]:
    """Same as before: load file & skip unwanted content"""
    print(f"Loading file: {file_path}")
    file_extension = os.path.splitext(file_path)[1].lower()
    try:
        if file_extension == '.pdf':
            loader = PyMuPDFLoader(file_path)
            documents = loader.load()
        elif file_extension in ['.md', '.markdown']:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            if re.search(r'不存在|风控', content):
                print(f"Skipping file {file_path} due to restricted keywords")
                return []
            loader = UnstructuredMarkdownLoader(file_path)
            documents = loader.load()
        elif file_extension in ['.txt', '.text', '.log']:
            loader = UnstructuredFileLoader(file_path)
            documents = loader.load()
        else:
            print(f"Unsupported file extension: {file_extension}")
            return []
        for doc in documents:
            doc.metadata['source'] = os.path.basename(file_path)
        return documents
    except Exception as e:
        print(f"Error loading file {file_path}: {str(e)}")
        return []


def create_or_update_db(doc_dir: str, persist_dir: str = "./data") -> Chroma:
    """
    Incrementally create or update a Chroma vector DB:
    - Loads existing DB if any
    - Only encodes and adds new files
    """
    # Ensure persist directory exists
    os.makedirs(persist_dir, exist_ok=True)

    # Load or initialize DB
    embedding_function = DeepSeekEmbeddingFunction()
    if os.path.exists(persist_dir) and os.path.exists(os.path.join(persist_dir, "chroma.sqlite3")):
        print("Loading existing DB...")
        vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding_function)
    else:
        print("Creating new DB...")
        vectordb = None

    # Get already indexed file names
    existing_sources = set()
    if vectordb is not None:
        try:
            # This returns all metadata, you can filter by 'source'
            for doc in vectordb.get()["metadatas"]:
                if "source" in doc:
                    existing_sources.add(doc["source"])
        except Exception as e:
            print(f"Error reading existing DB metadata: {e}")

    print(f"Already indexed files: {existing_sources}")

    # Load and split only new documents
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=150,
        length_function=len,
        is_separator_regex=False,
    )

    new_chunks = []
    for root, _, files in os.walk(doc_dir):
        for file in files:
            if file in existing_sources:
                print(f"Skipping already indexed file: {file}")
                continue
            docs = file_loader(os.path.join(root, file))
            if docs:
                chunks = text_splitter.split_documents(docs)
                new_chunks.extend(chunks)

    print(f"New chunks to add: {len(new_chunks)}")

    if not new_chunks:
        print("No new documents to process.")
        return vectordb

    if vectordb is None:
        # No existing DB, so build new
        vectordb = Chroma.from_documents(
            documents=new_chunks,
            embedding=embedding_function,
            persist_directory=persist_dir
        )
    else:
        # Append to existing DB
        vectordb.add_documents(new_chunks)

    vectordb.persist()
    print("DB persisted successfully.")
    return vectordb


def load_knowledge_db(persist_dir: str = "./data") -> Optional[Chroma]:
    """
    Load existing Chroma DB if exists
    """
    if not os.path.exists(persist_dir):
        print(f"No DB at {persist_dir}")
        return None
    embedding_function = DeepSeekEmbeddingFunction()
    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding_function)
    return vectordb

# 新增功能：RAG增强聊天

## 已完成功能

1. **RAG与聊天界面集成**
   - 修改了后端API，使聊天功能能够利用RAG系统
   - 更新了前端组件，支持传递课程ID和显示引用源
   - 实现了基于课程材料的智能问答功能

2. **文档和测试**
   - 创建了详细的设置指南 (`README_SETUP.md`)
   - 更新了集成指南 (`RAG_AI_Integration_Guide.md`)
   - 添加了测试脚本 (`test_rag_chat.py`)

## 使用方法

### 设置环境

1. 在RAG目录中创建`.env`文件，添加API密钥：
```
LLM_API_KEY='your-api-key'
LLM_API_BASE='https://api.siliconflow.cn/v1'
LLM_MODEL='deepseek-ai/DeepSeek-V3'
```

2. 安装依赖：
```bash
pip install -r requirements.txt
```

### 创建课程知识库

方法1：通过Web界面上传课程材料
- 登录系统
- 进入课程详情页
- 点击"课件资源"选项卡
- 上传文档（支持PDF、DOCX、TXT等格式）

方法2：通过命令行创建
```bash
cd RAG
python create_db.py --course_id <课程ID>
```

### 使用RAG聊天

1. 登录系统
2. 进入课程详情页
3. 点击"智能助手"选项卡
4. 提问与课程内容相关的问题

### 测试RAG功能

使用测试脚本验证RAG功能是否正常工作：
```bash
cd RAG
python test_rag_chat.py --course_id 003 --query "这门课程的主要内容是什么？"
```

## 下一步开发计划

1. **改进检索算法**
   - 实现重排序（reranking）以提高检索准确性
   - 添加语义搜索和关键词搜索的混合检索

2. **增强用户体验**
   - 在UI中显示更详细的引用信息
   - 添加反馈机制，让用户评价回答质量

3. **扩展功能**
   - 实现基于课程材料的自动出题功能
   - 添加个性化学习路径推荐

4. **性能优化**
   - 优化文档处理和索引创建过程
   - 实现缓存机制，减少重复查询的API调用

## 技术细节

### 主要组件

- **后端API** (`backend/api/rag_ai.py`)：处理聊天请求，集成RAG系统
- **前端组件** (`frontend/src/components/ai/AIAssistant.vue`)：提供聊天界面，传递课程上下文
- **RAG模块** (`RAG/rag_query.py`)：执行文档检索和查询处理

### 工作流程

1. 用户在特定课程的聊天界面中提问
2. 前端将问题和课程ID发送到后端API
3. 后端调用RAG系统，检索相关文档
4. 将检索到的文档与用户问题一起发送到LLM
5. LLM生成基于文档的回答
6. 后端将回答和引用源返回给前端
7. 前端显示回答和引用源

### 数据流

```
用户问题 → 前端 → 后端API → RAG检索 → LLM生成 → 回答+引用 → 前端显示
```

这个是怎么校验已存在文件的，通过哈希编码吗

询问如何校验,询问合理的文件结构,询问后期加入课程和融合数据库选项
优先处理较小文件并返回是否成功编码的状态