新增对已有文件的校验,

太好了！
那我来给你写一个「**自动增量更新版本**」的 `create_or_update_db`，思路是：

✅ 自动检测已有向量库
✅ 对比已有文档的 source（存在就跳过）
✅ 只对新增的文档做切块 + 调用 embedding API
✅ 用 `add_documents` 把新的块追加进去
✅ 避免重复嵌入，节省 token 和时间

---

## 🚀 **自动增量版**

请看下面完整示例（替换你现在的 `create_db` 和 `load_knowledge_db` 就行）：

```python
import os
import re
from typing import List, Dict, Optional
from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredMarkdownLoader, UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
import sys

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from embedding_util import get_embedding


class DeepSeekEmbeddingFunction:
    """Custom embedding function using DeepSeek API."""

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        result = get_embedding(texts)
        embeddings = [item["embedding"] for item in result["data"]]
        return embeddings

    def embed_query(self, text: str) -> List[float]:
        result = get_embedding(text)
        return result["data"][0]["embedding"]


def file_loader(file_path: str) -> List[Dict]:
    """Same as before: load file & skip unwanted content"""
    print(f"Loading file: {file_path}")
    file_extension = os.path.splitext(file_path)[1].lower()
    try:
        if file_extension == '.pdf':
            loader = PyMuPDFLoader(file_path)
            documents = loader.load()
        elif file_extension in ['.md', '.markdown']:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            if re.search(r'不存在|风控', content):
                print(f"Skipping file {file_path} due to restricted keywords")
                return []
            loader = UnstructuredMarkdownLoader(file_path)
            documents = loader.load()
        elif file_extension in ['.txt', '.text', '.log']:
            loader = UnstructuredFileLoader(file_path)
            documents = loader.load()
        else:
            print(f"Unsupported file extension: {file_extension}")
            return []
        for doc in documents:
            doc.metadata['source'] = os.path.basename(file_path)
        return documents
    except Exception as e:
        print(f"Error loading file {file_path}: {str(e)}")
        return []


def create_or_update_db(doc_dir: str, persist_dir: str = "./data") -> Chroma:
    """
    Incrementally create or update a Chroma vector DB:
    - Loads existing DB if any
    - Only encodes and adds new files
    """
    # Ensure persist directory exists
    os.makedirs(persist_dir, exist_ok=True)

    # Load or initialize DB
    embedding_function = DeepSeekEmbeddingFunction()
    if os.path.exists(persist_dir) and os.path.exists(os.path.join(persist_dir, "chroma.sqlite3")):
        print("Loading existing DB...")
        vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding_function)
    else:
        print("Creating new DB...")
        vectordb = None

    # Get already indexed file names
    existing_sources = set()
    if vectordb is not None:
        try:
            # This returns all metadata, you can filter by 'source'
            for doc in vectordb.get()["metadatas"]:
                if "source" in doc:
                    existing_sources.add(doc["source"])
        except Exception as e:
            print(f"Error reading existing DB metadata: {e}")

    print(f"Already indexed files: {existing_sources}")

    # Load and split only new documents
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=150,
        length_function=len,
        is_separator_regex=False,
    )

    new_chunks = []
    for root, _, files in os.walk(doc_dir):
        for file in files:
            if file in existing_sources:
                print(f"Skipping already indexed file: {file}")
                continue
            docs = file_loader(os.path.join(root, file))
            if docs:
                chunks = text_splitter.split_documents(docs)
                new_chunks.extend(chunks)

    print(f"New chunks to add: {len(new_chunks)}")

    if not new_chunks:
        print("No new documents to process.")
        return vectordb

    if vectordb is None:
        # No existing DB, so build new
        vectordb = Chroma.from_documents(
            documents=new_chunks,
            embedding=embedding_function,
            persist_directory=persist_dir
        )
    else:
        # Append to existing DB
        vectordb.add_documents(new_chunks)

    vectordb.persist()
    print("DB persisted successfully.")
    return vectordb


def load_knowledge_db(persist_dir: str = "./data") -> Optional[Chroma]:
    """
    Load existing Chroma DB if exists
    """
    if not os.path.exists(persist_dir):
        print(f"No DB at {persist_dir}")
        return None
    embedding_function = DeepSeekEmbeddingFunction()
    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding_function)
    return vectordb
```

---

## ⚡ **使用方式**

```bash
# 第一次
python your_script.py --doc_dir ./documents --db_dir ./data --action create

# 后续只要放新文件到 ./documents，然后再次执行
python your_script.py --doc_dir ./documents --db_dir ./data --action create
# 它只会对新文件做 embedding，老的文件自动跳过
```

---

## 🎉 **优点**

✅ 避免重复编码
✅ 同样文件只会被编码一次
✅ 不破坏已有的向量数据
✅ 可以持续追加

---

如果需要，我可以再帮你封装成一个更优雅的 CLI，或者加上日志记录，想要的话告诉我！🔥

这个是怎么校验已存在文件的，通过哈希编码吗

询问如何校验,询问合理的文件结构,询问后期加入课程和融合数据库选项
优先处理较小文件并返回是否成功编码的状态