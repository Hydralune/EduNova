æ–°å¢å¯¹å·²æœ‰æ–‡ä»¶çš„æ ¡éªŒ,

å¤ªå¥½äº†ï¼
é‚£æˆ‘æ¥ç»™ä½ å†™ä¸€ä¸ªã€Œ**è‡ªåŠ¨å¢é‡æ›´æ–°ç‰ˆæœ¬**ã€çš„ `create_or_update_db`ï¼Œæ€è·¯æ˜¯ï¼š

âœ… è‡ªåŠ¨æ£€æµ‹å·²æœ‰å‘é‡åº“
âœ… å¯¹æ¯”å·²æœ‰æ–‡æ¡£çš„ sourceï¼ˆå­˜åœ¨å°±è·³è¿‡ï¼‰
âœ… åªå¯¹æ–°å¢çš„æ–‡æ¡£åšåˆ‡å— + è°ƒç”¨ embedding API
âœ… ç”¨ `add_documents` æŠŠæ–°çš„å—è¿½åŠ è¿›å»
âœ… é¿å…é‡å¤åµŒå…¥ï¼ŒèŠ‚çœ token å’Œæ—¶é—´

---

## ğŸš€ **è‡ªåŠ¨å¢é‡ç‰ˆ**

è¯·çœ‹ä¸‹é¢å®Œæ•´ç¤ºä¾‹ï¼ˆæ›¿æ¢ä½ ç°åœ¨çš„ `create_db` å’Œ `load_knowledge_db` å°±è¡Œï¼‰ï¼š

```python
import os
import re
from typing import List, Dict, Optional
from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredMarkdownLoader, UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
import sys

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from embedding_util import get_embedding


class DeepSeekEmbeddingFunction:
    """Custom embedding function using DeepSeek API."""

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        result = get_embedding(texts)
        embeddings = [item["embedding"] for item in result["data"]]
        return embeddings

    def embed_query(self, text: str) -> List[float]:
        result = get_embedding(text)
        return result["data"][0]["embedding"]


def file_loader(file_path: str) -> List[Dict]:
    """Same as before: load file & skip unwanted content"""
    print(f"Loading file: {file_path}")
    file_extension = os.path.splitext(file_path)[1].lower()
    try:
        if file_extension == '.pdf':
            loader = PyMuPDFLoader(file_path)
            documents = loader.load()
        elif file_extension in ['.md', '.markdown']:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            if re.search(r'ä¸å­˜åœ¨|é£æ§', content):
                print(f"Skipping file {file_path} due to restricted keywords")
                return []
            loader = UnstructuredMarkdownLoader(file_path)
            documents = loader.load()
        elif file_extension in ['.txt', '.text', '.log']:
            loader = UnstructuredFileLoader(file_path)
            documents = loader.load()
        else:
            print(f"Unsupported file extension: {file_extension}")
            return []
        for doc in documents:
            doc.metadata['source'] = os.path.basename(file_path)
        return documents
    except Exception as e:
        print(f"Error loading file {file_path}: {str(e)}")
        return []


def create_or_update_db(doc_dir: str, persist_dir: str = "./data") -> Chroma:
    """
    Incrementally create or update a Chroma vector DB:
    - Loads existing DB if any
    - Only encodes and adds new files
    """
    # Ensure persist directory exists
    os.makedirs(persist_dir, exist_ok=True)

    # Load or initialize DB
    embedding_function = DeepSeekEmbeddingFunction()
    if os.path.exists(persist_dir) and os.path.exists(os.path.join(persist_dir, "chroma.sqlite3")):
        print("Loading existing DB...")
        vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding_function)
    else:
        print("Creating new DB...")
        vectordb = None

    # Get already indexed file names
    existing_sources = set()
    if vectordb is not None:
        try:
            # This returns all metadata, you can filter by 'source'
            for doc in vectordb.get()["metadatas"]:
                if "source" in doc:
                    existing_sources.add(doc["source"])
        except Exception as e:
            print(f"Error reading existing DB metadata: {e}")

    print(f"Already indexed files: {existing_sources}")

    # Load and split only new documents
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=150,
        length_function=len,
        is_separator_regex=False,
    )

    new_chunks = []
    for root, _, files in os.walk(doc_dir):
        for file in files:
            if file in existing_sources:
                print(f"Skipping already indexed file: {file}")
                continue
            docs = file_loader(os.path.join(root, file))
            if docs:
                chunks = text_splitter.split_documents(docs)
                new_chunks.extend(chunks)

    print(f"New chunks to add: {len(new_chunks)}")

    if not new_chunks:
        print("No new documents to process.")
        return vectordb

    if vectordb is None:
        # No existing DB, so build new
        vectordb = Chroma.from_documents(
            documents=new_chunks,
            embedding=embedding_function,
            persist_directory=persist_dir
        )
    else:
        # Append to existing DB
        vectordb.add_documents(new_chunks)

    vectordb.persist()
    print("DB persisted successfully.")
    return vectordb


def load_knowledge_db(persist_dir: str = "./data") -> Optional[Chroma]:
    """
    Load existing Chroma DB if exists
    """
    if not os.path.exists(persist_dir):
        print(f"No DB at {persist_dir}")
        return None
    embedding_function = DeepSeekEmbeddingFunction()
    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding_function)
    return vectordb
```

---

## âš¡ **ä½¿ç”¨æ–¹å¼**

```bash
# ç¬¬ä¸€æ¬¡
python your_script.py --doc_dir ./documents --db_dir ./data --action create

# åç»­åªè¦æ”¾æ–°æ–‡ä»¶åˆ° ./documentsï¼Œç„¶åå†æ¬¡æ‰§è¡Œ
python your_script.py --doc_dir ./documents --db_dir ./data --action create
# å®ƒåªä¼šå¯¹æ–°æ–‡ä»¶åš embeddingï¼Œè€çš„æ–‡ä»¶è‡ªåŠ¨è·³è¿‡
```

---

## ğŸ‰ **ä¼˜ç‚¹**

âœ… é¿å…é‡å¤ç¼–ç 
âœ… åŒæ ·æ–‡ä»¶åªä¼šè¢«ç¼–ç ä¸€æ¬¡
âœ… ä¸ç ´åå·²æœ‰çš„å‘é‡æ•°æ®
âœ… å¯ä»¥æŒç»­è¿½åŠ 

---

å¦‚æœéœ€è¦ï¼Œæˆ‘å¯ä»¥å†å¸®ä½ å°è£…æˆä¸€ä¸ªæ›´ä¼˜é›…çš„ CLIï¼Œæˆ–è€…åŠ ä¸Šæ—¥å¿—è®°å½•ï¼Œæƒ³è¦çš„è¯å‘Šè¯‰æˆ‘ï¼ğŸ”¥

è¿™ä¸ªæ˜¯æ€ä¹ˆæ ¡éªŒå·²å­˜åœ¨æ–‡ä»¶çš„ï¼Œé€šè¿‡å“ˆå¸Œç¼–ç å—

è¯¢é—®å¦‚ä½•æ ¡éªŒ,è¯¢é—®åˆç†çš„æ–‡ä»¶ç»“æ„,è¯¢é—®åæœŸåŠ å…¥è¯¾ç¨‹å’Œèåˆæ•°æ®åº“é€‰é¡¹
ä¼˜å…ˆå¤„ç†è¾ƒå°æ–‡ä»¶å¹¶è¿”å›æ˜¯å¦æˆåŠŸç¼–ç çš„çŠ¶æ€